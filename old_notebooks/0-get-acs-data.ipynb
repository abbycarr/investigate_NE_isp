{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Census Data\n",
    "Collects ACS data from census.gov API and merges it into one table.\n",
    "\n",
    "If you want to collect your own census data, you'll need to regiester for an [API key](https://api.census.gov/data/key_signup.html), and assign it as the environment variable `CENSUS_API_KEY`. \n",
    "\n",
    "This is not necessary, as all outputs are calcualted and saved in this repository already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import glob as glob\n",
    "import gzip\n",
    "import multiprocess\n",
    "from multiprocess import Pool\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download ACS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouputs\n",
    "data_dir = '../data/input/census/acs5'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# inputs\n",
    "fn_income = '../data/census/state2income.json'\n",
    "input_files = glob.glob('../data/intermediary/isp/*/*/*.geojson.gz')\n",
    "fips = set([_.split('/')[-1].split('.geojson')[0][:5] for _ in input_files])\n",
    "fips = set([_ for _ in fips if _ != 'spotc'])\n",
    "state2income = json.load(open(fn_income, 'r'))\n",
    "\n",
    "# params\n",
    "recalculate = False\n",
    "n_jobs = 8\n",
    "API_KEY = os.environ.get('CENSUS_API_KEY')\n",
    "# assert API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs\n",
    "fn_out_acs = '../data/intermediary/census/aggregated_tables.csv.gz'\n",
    "os.makedirs(os.path.dirname(fn_out_acs), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the three ACS tables and columns we're going to get.\n",
    "acs_tables = [\n",
    "    {\n",
    "        \"display_name\": \"race_ethnicity\",\n",
    "        \"table_name\": \"B03002\",\n",
    "        \"url\": \"https://api.census.gov/data/2018/acs/acs5/groups/B03002.html\",\n",
    "        \"columns\": {\n",
    "            \"block group\": \"block_group\",\n",
    "            \"B03002_001E\": \"race_total_estimate\",\n",
    "            \"B03002_003E\": \"race_white_alone\",\n",
    "            \"B03002_004E\": \"race_black_alone\",\n",
    "            \"B03002_005E\": \"race_aindian_alone\",\n",
    "            \"B03002_006E\": \"race_asian_alone\",\n",
    "            \"B03002_007E\": \"race_pacific_islander_native_hawaiian_alone\",\n",
    "            \"B03002_008E\": \"race_some_other_alone\",\n",
    "            \"B03002_009E\": \"race_two_or_more_alone\",\n",
    "            \"B03002_012E\": \"race_latino_alone\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"display_name\": \"household_income_2\",\n",
    "        \"table_name\": 'B19013',\n",
    "        \"url\": \"https://api.census.gov/data/2018/acs/acs5/groups/B19013.html\",\n",
    "        \"columns\": {\n",
    "            \"block group\": \"block_group\",\n",
    "            \"B19013_001E\": \"median_household_income\",\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"display_name\": \"internet_subscription\",\n",
    "        \"table_name\": 'B28002',\n",
    "        \"url\": \"https://api.census.gov/data/2018/acs/acs5/groups/B28002.html\",\n",
    "        \"columns\": {\n",
    "            \"block group\": \"block_group\",\n",
    "            \"B28002_001E\": \"internet_total_estimate\",\n",
    "            \"B28002_002E\": \"internet_subscriptions_any\",\n",
    "            \"B28002_004E\": \"internet_broadband\",\n",
    "            \"B28002_005E\": \"internet_mobile\",\n",
    "            \"B28002_006E\": \"internet_mobile_only\",\n",
    "            \"B28002_013E\": \"internet_no_access\",\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_acs(column: str= \"B03002_001E\",\n",
    "                 geography: str = \"block group\",\n",
    "                 year: int= 2019, \n",
    "                 state: int= 55,\n",
    "                 county: [int, str]= 1,\n",
    "                 data_dir: str= 'data/',\n",
    "                 api_key: str= None,\n",
    "                 debug: bool= False) -> dict:\n",
    "    \"\"\"\n",
    "    geography can also be \"tract\",\n",
    "    column is the ACS \"table name\"\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    \n",
    "    def make_acs_request(year, column, geography, state, county, api_key, debug=False):\n",
    "        \"\"\"\n",
    "        Formats the API call and make the reuqest\n",
    "        \"\"\"\n",
    "        county = str(county).zfill(3)\n",
    "        url = (f\"https://api.census.gov/data/{year}/acs/acs5?get={column}&\"\n",
    "               f\"for={geography}:*&in=state:{state}%20county:{county}&key={api_key}\")\n",
    "        if debug:\n",
    "            print(url)\n",
    "        return requests.get(url)\n",
    "\n",
    "    \n",
    "    # check if the file exists...\n",
    "    table = column.split('_')[0]\n",
    "    geography_ = geography.replace(' ', '_')\n",
    "    fn_out = f\"{data_dir}/{geography_}/{year}/{state}/{county}/{table}/{column}.csv.gz\"\n",
    "    if os.path.exists(fn_out):\n",
    "        return 1\n",
    "    os.makedirs(os.path.dirname(fn_out), exist_ok=True)\n",
    "    print(f\"collecting {fn_out}\")\n",
    "    # make the request\n",
    "    resp = make_acs_request(year, column, geography, state, county, api_key, debug)\n",
    "    \n",
    "    # validate the response and save it\n",
    "    if resp.status_code != 200:\n",
    "        pd.DataFrame([]).to_csv(fn_out, index=False, compression='gzip')\n",
    "        return 0\n",
    "    _data = resp.json()\n",
    "    _df = pd.DataFrame(_data[1:], columns=_data[0])\n",
    "    _df.to_csv(fn_out, index=False, compression='gzip')\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_acs_request(year, column, geography, state, county, api_key, debug=False):\n",
    "        \"\"\"\n",
    "        Formats the API call and make the reuqest\n",
    "        \"\"\"\n",
    "        county = str(county).zfill(3)\n",
    "        url = (f\"https://api.census.gov/data/{year}/acs/acs5?get={column}&\"\n",
    "               f\"for={geography}:*&in=state:{state}%20county:{county}&key={api_key}\")\n",
    "        if debug:\n",
    "            print(url)\n",
    "        return requests.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed things up, we're going to use multiprocessing to collect these files.\n",
    "First, we must create a list of arguments for each API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We must make 16 API calls.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2019\n",
    "debug = False\n",
    "args = []\n",
    "for fip in fips:\n",
    "    state = fip[:2]\n",
    "    county = fip[2:]\n",
    "    for geography in ['block group']:\n",
    "        for table in acs_tables:\n",
    "            table_name = table[\"display_name\"]\n",
    "            columns = [c for c in table[\"columns\"].keys() if c != \"block group\"]\n",
    "            for column in columns:\n",
    "                args.append([column, geography, year, state, county, data_dir, API_KEY, debug])\n",
    "\n",
    "f\"We must make {len(args)} API calls.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use multiprocessing to make the requests...\n",
    "def collect_data():\n",
    "    with multiprocess.get_context(\"spawn\").Pool(n_jobs) as pool:\n",
    "        pool.starmap(download_acs, tqdm(args, total=len(args)))\n",
    "        \n",
    "# only make API requests if the output file doesn't exists or if we want to recalculate.\n",
    "if not os.path.exists(fn_out_acs) or recalculate:\n",
    "    collect_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "With the ACS data collected, we're now going to calculate the racial demographics, income levels, and broadband pentration for each block group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_block_groups(verbose=False):\n",
    "    \"\"\"Get the block groups in the data we have\"\"\"\n",
    "    files_input = glob.glob(\"../data/input/isp/*/*/*.geojson.gz\")    \n",
    "    block_groups = set([f.split('/')[-1].split('.geojson')[0] for f in files_input])\n",
    "    block_groups = set([bg for bg in block_groups if len(bg) == 12])\n",
    "    if verbose:\n",
    "        print(f\"Found {len(block_groups)} block groups.\")\n",
    "    \n",
    "    return block_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoid(row):\n",
    "    \"\"\"\n",
    "    Explictly cast block group-level geoid\n",
    "    \"\"\"\n",
    "    state = int(row['state'])\n",
    "    county = int(row['county'])\n",
    "    tract = int(row['tract'])\n",
    "    block = int(row['block_group'])\n",
    "    \n",
    "    return f\"{state:02}{county:03}{tract:06}{block:01}\"\n",
    "\n",
    "def percent_non_white(row):\n",
    "    \"\"\"\n",
    "    Percentage of area that are not non-Hispanic white\n",
    "    \"\"\"\n",
    "    total = row['race_total_estimate']\n",
    "    white = row['race_white_alone']\n",
    "    try:\n",
    "        perc_non_white =  (total - white) / total\n",
    "        return max(0, min(perc_non_white, 1))\n",
    "    except ZeroDivisionError:\n",
    "        return None\n",
    "    \n",
    "def income_level(row):\n",
    "    \"\"\"\n",
    "    Calculate median household income compared to city median income.\n",
    "    We set None for skip values set by the census bureau.\n",
    "    \"\"\"\n",
    "    state = row['state']\n",
    "    if row['median_household_income'] == -666666666:\n",
    "        return None\n",
    "    median_income = state2income.get(str(state), {})\n",
    "    if median_income:\n",
    "        return row['median_household_income'] / median_income\n",
    "    return None\n",
    "\n",
    "def dollars_diff_median(row):\n",
    "    \"\"\"\n",
    "    Get dollars below median city income.\n",
    "    \"\"\"\n",
    "    state = row['state']\n",
    "    if row['median_household_income'] == -666666666:\n",
    "        return None\n",
    "    median_income = state2income.get(str(state), {})\n",
    "    if median_income:\n",
    "        return median_income - row['median_household_income']\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_out_acs) or recalculate:\n",
    "    # This merges and concats each ACS table we collected.\n",
    "    geography = \"block group\"\n",
    "    df = pd.DataFrame([])\n",
    "    for table in acs_tables:\n",
    "        col2rename = table['columns']\n",
    "        table_name = table['table_name']\n",
    "        cols = [c for c in col2rename.keys() if c != \"block group\"]\n",
    "        for column in tqdm(cols):\n",
    "            files = glob.glob(f\"../data/input/census/acs5/{geography.replace(' ', '_')}/{year}/*/*/{table_name}/{column}.csv.gz\")\n",
    "            data = []\n",
    "            for fn in files:\n",
    "                tp = pd.read_csv(fn, compression='gzip')\n",
    "                if tp.empty:\n",
    "                    print(fn)\n",
    "                data.extend(tp.to_dict(\"records\"))\n",
    "            tmp = pd.DataFrame(data)\n",
    "            tmp.columns = [col2rename.get(c, c) for c in tmp.columns]\n",
    "            if df.empty:\n",
    "                df = tmp\n",
    "            else:\n",
    "                df = df.merge(tmp, on= [\"state\", \"county\", \"tract\", \"block_group\"], how='outer')\n",
    "\n",
    "    # process the data\n",
    "    df['race_perc_non_white'] = df.apply(percent_non_white, axis=1)\n",
    "    df['internet_perc_broadband'] = df['internet_broadband'] / df['internet_total_estimate']\n",
    "    df['geoid'] = df.apply(get_geoid, axis=1)\n",
    "    \n",
    "    # For whatever reason, calling these functions can return None.\n",
    "    # check how many null values they produce, and run again if there are a lot of nulls.\n",
    "    df['income_dollars_below_median'] = df.apply(dollars_diff_median, axis=1)\n",
    "    df['income_lmi'] = df.apply(income_level, axis=1)\n",
    "    \n",
    "    df.to_csv(fn_out_acs, index=False, compression='gzip')\n",
    "else:\n",
    "    df = pd.read_csv(fn_out_acs, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing raw shape file for maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TIGER](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.2019.html) shape files were downloaded and stored in one place (`../data/input/census/shape/raw`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ouputs\n",
    "fn_out_shp = '../data/intermediary/census/2019_acs_5_shapes.geojson.gz'\n",
    "pattern_geojson = '../data/input/census/shape/preprocessed/*.geojson'\n",
    "\n",
    "# inputs\n",
    "files_shape = glob.glob('../data/input/census/shape/raw/tl_2019_*_bg.zip')\n",
    "len(files_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to filter out block groups that aren't in our investigation, merge ACS demographic data, and combine the shapes into one file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the zip files need to be converted into geoJSON (via geopandas `gpd`) to map-able."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/46 [02:39<38:07, 53.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\J-Dog\\23FALL\\Algorithm Audits\\investigate_NE_isp\\old_notebooks\\0-get-acs-data.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/old_notebooks/0-get-acs-data.ipynb#X31sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# shp = gpd.read_file('zip:///' + fn)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/old_notebooks/0-get-acs-data.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m shp \u001b[39m=\u001b[39m gpd\u001b[39m.\u001b[39mread_file(fn)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/old_notebooks/0-get-acs-data.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m shp\u001b[39m.\u001b[39;49mto_file(fn_out, driver\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mGeoJSON\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geopandas\\geodataframe.py:1264\u001b[0m, in \u001b[0;36mGeoDataFrame.to_file\u001b[1;34m(self, filename, driver, schema, index, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m \n\u001b[0;32m   1175\u001b[0m \u001b[39mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1260\u001b[0m \n\u001b[0;32m   1261\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1262\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgeopandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfile\u001b[39;00m \u001b[39mimport\u001b[39;00m _to_file\n\u001b[1;32m-> 1264\u001b[0m _to_file(\u001b[39mself\u001b[39m, filename, driver, schema, index, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geopandas\\io\\file.py:612\u001b[0m, in \u001b[0;36m_to_file\u001b[1;34m(df, filename, driver, schema, index, mode, crs, engine, **kwargs)\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should be one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    611\u001b[0m \u001b[39mif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfiona\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 612\u001b[0m     _to_file_fiona(df, filename, driver, schema, crs, mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    613\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyogrio\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    614\u001b[0m     _to_file_pyogrio(df, filename, driver, schema, crs, mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geopandas\\io\\file.py:641\u001b[0m, in \u001b[0;36m_to_file_fiona\u001b[1;34m(df, filename, driver, schema, crs, mode, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m     crs_wkt \u001b[39m=\u001b[39m crs\u001b[39m.\u001b[39mto_wkt(\u001b[39m\"\u001b[39m\u001b[39mWKT1_GDAL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    638\u001b[0m \u001b[39mwith\u001b[39;00m fiona\u001b[39m.\u001b[39mopen(\n\u001b[0;32m    639\u001b[0m     filename, mode\u001b[39m=\u001b[39mmode, driver\u001b[39m=\u001b[39mdriver, crs_wkt\u001b[39m=\u001b[39mcrs_wkt, schema\u001b[39m=\u001b[39mschema, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    640\u001b[0m ) \u001b[39mas\u001b[39;00m colxn:\n\u001b[1;32m--> 641\u001b[0m     colxn\u001b[39m.\u001b[39;49mwriterecords(df\u001b[39m.\u001b[39;49miterfeatures())\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\fiona\\collection.py:558\u001b[0m, in \u001b[0;36mCollection.writerecords\u001b[1;34m(self, records)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcollection not open for writing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 558\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mwriterecs(records, \u001b[39mself\u001b[39;49m)\n\u001b[0;32m    559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_len \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession\u001b[39m.\u001b[39mget_length()\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mfiona\\\\ogrext.pyx:1392\u001b[0m, in \u001b[0;36mfiona.ogrext.WritingSession.writerecs\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geopandas\\geodataframe.py:951\u001b[0m, in \u001b[0;36mGeoDataFrame.iterfeatures\u001b[1;34m(self, na, show_bbox, drop_id)\u001b[0m\n\u001b[0;32m    949\u001b[0m feature[\u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    950\u001b[0m feature[\u001b[39m\"\u001b[39m\u001b[39mproperties\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m properties_items\n\u001b[1;32m--> 951\u001b[0m feature[\u001b[39m\"\u001b[39m\u001b[39mgeometry\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m mapping(geom) \u001b[39mif\u001b[39;00m geom \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39mif\u001b[39;00m show_bbox:\n\u001b[0;32m    954\u001b[0m     feature[\u001b[39m\"\u001b[39m\u001b[39mbbox\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m geom\u001b[39m.\u001b[39mbounds \u001b[39mif\u001b[39;00m geom \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shapely\\geometry\\geo.py:135\u001b[0m, in \u001b[0;36mmapping\u001b[1;34m(ob)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmapping\u001b[39m(ob):\n\u001b[0;32m    116\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    Returns a GeoJSON-like mapping from a Geometry or any\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39m    object which implements __geo_interface__\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m    {'type': 'Point', 'coordinates': (0.0, 0.0)}\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m ob\u001b[39m.\u001b[39;49m__geo_interface__\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shapely\\geometry\\polygon.py:291\u001b[0m, in \u001b[0;36mPolygon.__geo_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__geo_interface__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexterior \u001b[39m==\u001b[39m LinearRing():\n\u001b[0;32m    292\u001b[0m         coords \u001b[39m=\u001b[39m []\n\u001b[0;32m    293\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shapely\\geometry\\polygon.py:246\u001b[0m, in \u001b[0;36mPolygon.exterior\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexterior\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m shapely\u001b[39m.\u001b[39;49mget_exterior_ring(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     78\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m arr, old_flag \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\shapely\\_geometry.py:425\u001b[0m, in \u001b[0;36mget_exterior_ring\u001b[1;34m(geometry, **kwargs)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[39m@multithreading_enabled\u001b[39m\n\u001b[0;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exterior_ring\u001b[39m(geometry, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    405\u001b[0m     \u001b[39m\"\"\"Returns the exterior ring of a polygon.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \n\u001b[0;32m    407\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mget_exterior_ring(geometry, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# convert zip files to geoJSON\n",
    "for fn in tqdm(files_shape):\n",
    "    fn = os.path.abspath(fn)\n",
    "    fn_out = fn.replace('/raw/', '/preprocessed/').replace('.zip', '.geojson')\n",
    "    if os.path.exists(fn_out):\n",
    "        continue\n",
    "    os.makedirs(os.path.dirname(fn_out), exist_ok=True)\n",
    "    # shp = gpd.read_file('zip:///' + fn)\n",
    "    shp = gpd.read_file(fn)\n",
    "    shp.to_file(fn_out, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_out_shp) or recalculate:\n",
    "    df = pd.read_csv(fn_out_acs, compression='gzip')\n",
    "    df.geoid = df.geoid.apply(lambda x: f\"{x:012d}\")\n",
    "\n",
    "    block_groups = get_relevant_block_groups(verbose=True)    \n",
    "    geoid2meta = {row['geoid']: {\n",
    "        'race_perc_non_white' : row['race_perc_non_white'], \n",
    "        'income_dollars_below_median' : row['income_dollars_below_median'],\n",
    "        'median_household_income' : row['median_household_income']\n",
    "    }  for i, row in df[df.geoid.isin(block_groups)].iterrows()}\n",
    "    len(geoid2meta)\n",
    "    \n",
    "    save_all = []\n",
    "    files_geojson = glob.glob(pattern_geojson)\n",
    "    geography = \"block group\"\n",
    "    for fn_geojson in tqdm(files_geojson):\n",
    "        fn_out =  f'../data/intermediary/maps/acs/{os.path.basename(fn_geojson)}'\n",
    "        if not os.path.exists(fn_out) or recalculate:\n",
    "            os.makedirs(os.path.dirname(fn_out), exist_ok=True)\n",
    "            to_save = []\n",
    "            with open(fn_geojson , 'r') as f:\n",
    "                geojson = json.load(f)\n",
    "                for i, feature in enumerate(geojson['features']):\n",
    "                    featureProperties = feature['properties']\n",
    "                    if geography == \"block group\":\n",
    "                        geoid = featureProperties['GEOID']\n",
    "                    featureData = geoid2meta.get(geoid, {})\n",
    "                    for key in featureData.keys():\n",
    "                        featureProperties[key] = featureData[key]\n",
    "                    if featureData:\n",
    "                        to_save.append(feature)\n",
    "                        save_all.append(feature)\n",
    "\n",
    "                # save geoJSON with acs data merged in\n",
    "                geojson['features'] = to_save\n",
    "                with open(fn_out, 'w') as f:\n",
    "                    f.write(json.dumps(geojson).replace('NaN', '\"\"'))\n",
    "    geojson['features'] = save_all\n",
    "    with gzip.open(fn_out_shp, 'wt') as f:\n",
    "        f.write(json.dumps(geojson).replace('NaN', '\"\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## population density & number of competing ISPs\n",
    "Final tweaks to the ACS data to be used for analysis.\n",
    "\n",
    "We reference TIGER shape files to calculate population density using the the area of land for each census block group and the estimated population. We also merge the number of competing ISPs for each block group according to the FCC's Form 477."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouputs\n",
    "fn_out_features = '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'\n",
    "fn_providers = '../data/intermediary/fcc/bg_providers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "fn_form_477 = '../data/input/fcc/fbd_us_with_satellite_dec2020_v1.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of competitors from FCC Form 477.\n",
    "if not os.path.exists(fn_providers) or recalculate:\n",
    "    # Filter for block groups in our invetigation.\n",
    "    block_groups = get_relevant_block_groups(verbose=True)\n",
    "    data = []\n",
    "    for _df in pd.read_csv(fn_form_477, \n",
    "                           compression= 'gzip',\n",
    "                           encoding= 'unicode_escape', \n",
    "                           chunksize= 50000, \n",
    "                           dtype= {'BlockCode' : str}):\n",
    "        _df['block_group'] = _df['BlockCode'].apply(lambda x: x[:12])\n",
    "        data.extend(_df[_df['block_group'].isin(block_groups)]\n",
    "                                          .to_dict(orient='records'))\n",
    "    df_bg = pd.DataFrame(data)\n",
    "    \n",
    "    (df_bg[\n",
    "        (df_bg.Consumer == 1) &\n",
    "        (~df_bg.TechCode.isin([60,70]))   \n",
    "    ].groupby('block_group')['ProviderName']\n",
    "     .nunique().to_frame('n_providers')\n",
    "     .to_csv(fn_providers)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_persons_per_sq_mile(row):\n",
    "    \"\"\"\n",
    "    See formula referenced in these two sources:\n",
    "    https://acsdatacommunity.prb.org/discussion-forum/f/forum/585/moe-for-population-density\n",
    "    https://www.census.gov/quickfacts/fact/note/US/LND110210\n",
    "    1,000,000 * POP / ALAND\n",
    "    \"\"\"\n",
    "    if row['ALAND'] == 0:\n",
    "        return None\n",
    "    sq_miles = row['ALAND'] / 1000000\n",
    "    persons = row['race_total_estimate']\n",
    "    return persons / sq_miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_out_features) or recalculate:\n",
    "    df = pd.read_csv(fn_out_acs, compression='gzip', dtype={'geoid': str})\n",
    "    df_shp = pd.DataFrame([row['properties'] for row in \n",
    "                           json.load(gzip.open(fn_out_shp, 'r'))['features']])\n",
    "    df_providers = pd.read_csv(fn_providers, dtype={'block_group': str})\n",
    "    \n",
    "    df = df.merge(df_shp[['GEOID', 'ALAND']], \n",
    "                  how='left', left_on='geoid', right_on='GEOID', \n",
    "                  suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\n",
    "    df['ppl_per_sq_mile'] = df.apply(calculate_persons_per_sq_mile, axis=1)\n",
    "\n",
    "    df = df.merge(df_providers, \n",
    "                  how='left', left_on='GEOID', right_on='block_group', \n",
    "                  suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\n",
    "    \n",
    "    df.to_csv(fn_out_features, index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
