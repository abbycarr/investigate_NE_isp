{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Processing Lookup Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import multiprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from parsers import (\n",
    "    isp_workflow,\n",
    "    get_incorporated_places, \n",
    "    check_redlining, \n",
    "    get_holc_grade, \n",
    "    get_closest_fiber\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "fn_acs = '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'\n",
    "pattern_hughes = '../data/intermediary/isp/hughes/*/*.geojson.gz' # pattern for all data collected from lookup tools\n",
    "pattern_xfinity = '../data/intermediary/isp/xfinity/*/*.geojson.gz'\n",
    "pattern_viastat =  \"../data/intermediary/isp/viastat/*/*.geojson.gz\"\n",
    "\n",
    "# outputs\n",
    "fn_hughes = \"../data/output/speed_price_hughes.csv.gz\"\n",
    "fn_xfinity = '../data/output/speed_price_xfinity.csv.gz'\n",
    "fn_viastat = '../data/output/speed_price_viastat.csv.gz'\n",
    "\n",
    "# params\n",
    "n_jobs = 20\n",
    "recalculate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from Census data we crunched in the previous notebook.\n",
    "acs = pd.read_csv(fn_acs, dtype={'geoid': str, 'block_group': str})\n",
    "\n",
    "# These are the columns we're going to bring to merge with lookup responses.\n",
    "acs_cols = [\n",
    "    'geoid', 'race_perc_non_white','income_lmi', \n",
    "    'ppl_per_sq_mile', 'n_providers', 'income_dollars_below_median',\n",
    "    'internet_perc_broadband', 'median_household_income'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_addresses(fn):\n",
    "    \"\"\"\n",
    "    How many addresses did we successfully collect in each file?\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    import json\n",
    "    count = 0\n",
    "    with gzip.open(fn, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            record = json.loads(line)\n",
    "            count += 1\n",
    "    return count \n",
    "\n",
    "def count_successful_addresses(pattern, n_jobs=20):\n",
    "    \"\"\"\n",
    "    For all files in `pattern`, sees how many addresses were successfully counted.\n",
    "    Uses multiprocessing to speed things up.\n",
    "    \"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    count = 0\n",
    "    with multiprocess.get_context(\"spawn\").Pool(n_jobs) as pool:\n",
    "        for _count in tqdm(pool.imap_unordered(count_addresses, files), \n",
    "                           total=len(files)):\n",
    "            count += _count\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 24.03it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.88it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hughes Net: 13\n",
      "Xfinity: 5\n",
      "ViaStat: 0\n",
      "Total: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hughes_count = count_successful_addresses(pattern_hughes, n_jobs=n_jobs)\n",
    "xfinity_count = count_successful_addresses(pattern_xfinity, n_jobs=n_jobs)\n",
    "viastat_count = count_successful_addresses(pattern_viastat, n_jobs=n_jobs)\n",
    "all_records = hughes_count + xfinity_count + viastat_count\n",
    "\n",
    "print(f\"\"\"Hughes Net: {hughes_count}\n",
    "Xfinity: {xfinity_count}\n",
    "ViaStat: {viastat_count}\n",
    "Total: {all_records}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions we're going to be using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `check_redlining` grades by looking if an addresses' coordinates (converted to a Shapely `Point`) are within the `Polygon`s of redlining maps by Mapping Inequality. This actual check is done by `get_holc_grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mget_holc_grade\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolygons\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSource:\u001b[0m   \n",
      "\u001b[1;32mdef\u001b[0m \u001b[0mget_holc_grade\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m\n",
      "\u001b[0m                   \u001b[0mpolygons\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"\"\"\n",
      "    Converts any lat and lon in a dictionary into a shapely point,\n",
      "    then iterate through a list of dictionaries containing \n",
      "    shapely polygons shapes for each HOLC-graded area.\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mpolygon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpolygons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'grade'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\j-dog\\23fall\\algorithm audits\\investigate_ne_isp\\notebooks\\parsers.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "??get_holc_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hughes Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:12<00:00,  1.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 11.43it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_hughes) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_hughes = []\n",
    "    files = glob.glob(pattern_hughes)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `hughes_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_hughes.extend(record)\n",
    "    hughes = pd.DataFrame(data_hughes)\n",
    "    del data_hughes\n",
    "    \n",
    "    # commented out, did not seem necessary to merge\n",
    "    # hughes['block_group'] = hughes['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    hughes = check_redlining(hughes)\n",
    "    # merge census data, and save the file\n",
    "    hughes_acs = hughes.merge(acs[acs_cols], how='left',\n",
    "                        left_on='geoid', right_on='geoid')\n",
    "    hughes_acs = hughes_acs[[c for c in hughes_acs.columns if c != 'geoid']]\n",
    "    hughes_acs.to_csv(fn_hughes, index=False, compression='gzip')\n",
    "else:\n",
    "    hughes_acs = pd.read_csv(fn_hughes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2023, 12, 5, 9, 42, 43, 889807),\n",
       " datetime.datetime(2023, 12, 5, 9, 44, 52, 806839)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(hughes_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(hughes_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hughes_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(hughes_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redlining_grade\n",
       "C    0.636364\n",
       "D    0.363636\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hughes_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xfinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:07<00:00,  1.60s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 40.52it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_xfinity) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_xfinity = []\n",
    "    files = glob.glob(pattern_xfinity)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `isp_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_xfinity.extend(record)\n",
    "    xfinity = pd.DataFrame(data_xfinity)\n",
    "    del data_xfinity\n",
    "    \n",
    "    \n",
    "    xfinity['block_group'] = xfinity['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    xfinity = check_redlining(xfinity)\n",
    "    # merge census data, and save the file\n",
    "    xfinity_acs = xfinity.merge(acs[acs_cols], how='left',\n",
    "                        left_on='geoid', right_on='geoid')\n",
    "    xfinity_acs = xfinity_acs[[c for c in xfinity_acs.columns if c != 'geoid']]\n",
    "    xfinity_acs.to_csv(fn_xfinity, index=False, compression='gzip')\n",
    "else:\n",
    "    xfinity_acs = pd.read_csv(fn_xfinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2023, 12, 5, 9, 43, 3, 302809),\n",
       " datetime.datetime(2023, 12, 5, 9, 44, 32, 87669)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(xfinity_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(xfinity_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xfinity_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(xfinity_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redlining_grade\n",
       "C    0.666667\n",
       "D    0.333333\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfinity_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viastat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'block_group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\J-Dog\\23FALL\\Algorithm Audits\\investigate_NE_isp\\notebooks\\1-process-offers.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y146sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m viastat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data_viastat)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y146sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdel\u001b[39;00m data_viastat\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y146sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m viastat[\u001b[39m'\u001b[39m\u001b[39mblock_group\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m viastat[\u001b[39m'\u001b[39;49m\u001b[39mblock_group\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(x)\u001b[39m:\u001b[39;00m\u001b[39m012d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y146sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y146sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m viastat \u001b[39m=\u001b[39m check_redlining(viastat)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 418\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m    419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    420\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'block_group'"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_viastat) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_viastat = []\n",
    "    files = glob.glob(pattern_viastat)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `isp_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_viastat.extend(record)\n",
    "    viastat = pd.DataFrame(data_viastat)\n",
    "    del data_viastat\n",
    "    \n",
    "    \n",
    "    viastat['block_group'] = viastat['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    viastat = check_redlining(viastat)\n",
    "    # merge census data, and save the file\n",
    "    viastat_acs = viastat.merge(acs[acs_cols], how='left',\n",
    "                        left_on='geoid', right_on='geoid')\n",
    "    viastat_acs = viastat_acs[[c for c in viastat_acs.columns if c != 'geoid']]\n",
    "    viastat_acs.to_csv(fn_viastat, index=False, compression='gzip')\n",
    "else:\n",
    "    viastat_acs = pd.read_csv(fn_viastat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'viastat_acs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\J-Dog\\23FALL\\Algorithm Audits\\investigate_NE_isp\\notebooks\\1-process-offers.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y141sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# start and end collection datetime\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y141sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m [datetime\u001b[39m.\u001b[39mfromtimestamp(viastat_acs\u001b[39m.\u001b[39mcollection_datetime\u001b[39m.\u001b[39mmin()), \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y141sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m  datetime\u001b[39m.\u001b[39mfromtimestamp(viastat_acs\u001b[39m.\u001b[39mcollection_datetime\u001b[39m.\u001b[39mmax())]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'viastat_acs' is not defined"
     ]
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(viastat_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(viastat_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'viastat_acs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\J-Dog\\23FALL\\Algorithm Audits\\investigate_NE_isp\\notebooks\\1-process-offers.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y142sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mlen\u001b[39m(viastat_acs)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'viastat_acs' is not defined"
     ]
    }
   ],
   "source": [
    "len(viastat_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(viastat_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viastat_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
