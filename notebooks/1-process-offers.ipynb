{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Processing Lookup Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import multiprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from parsers import (\n",
    "    isp_workflow,\n",
    "    get_incorporated_places, \n",
    "    check_redlining, \n",
    "    get_holc_grade, \n",
    "    get_closest_fiber\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "fn_acs = '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'\n",
    "pattern_hughes = '../data/intermediary/isp/hughes/*/*.geojson.gz' # pattern for all data collected from lookup tools\n",
    "pattern_xfinity = '../data/intermediary/isp/xfinity/*/*.geojson.gz'\n",
    "pattern_viastat =  \"../data/intermediary/isp/viastat/*/*.geojson.gz\"\n",
    "\n",
    "# outputs\n",
    "fn_hughes = \"../data/output/speed_price_hughes.csv.gz\"\n",
    "fn_xfinity = '../data/output/speed_price_xfinity.csv.gz'\n",
    "fn_viastat = '../data/output/speed_price_viastat.csv.gz'\n",
    "\n",
    "# params\n",
    "n_jobs = 20\n",
    "recalculate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from Census data we crunched in the previous notebook.\n",
    "acs = pd.read_csv(fn_acs, dtype={'geoid': str, 'block_group': str})\n",
    "\n",
    "# These are the columns we're going to bring to merge with lookup responses.\n",
    "acs_cols = [\n",
    "    'geoid', 'race_perc_non_white','income_lmi', \n",
    "    'ppl_per_sq_mile', 'n_providers', 'income_dollars_below_median',\n",
    "    'internet_perc_broadband', 'median_household_income'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_addresses(fn):\n",
    "    \"\"\"\n",
    "    How many addresses did we successfully collect in each file?\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    import json\n",
    "    count = 0\n",
    "    with gzip.open(fn, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            record = json.loads(line)\n",
    "            count += 1\n",
    "    return count \n",
    "\n",
    "def count_successful_addresses(pattern, n_jobs=20):\n",
    "    \"\"\"\n",
    "    For all files in `pattern`, sees how many addresses were successfully counted.\n",
    "    Uses multiprocessing to speed things up.\n",
    "    \"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    count = 0\n",
    "    with multiprocess.get_context(\"spawn\").Pool(n_jobs) as pool:\n",
    "        for _count in tqdm(pool.imap_unordered(count_addresses, files), \n",
    "                           total=len(files)):\n",
    "            count += _count\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hughes_count = count_successful_addresses(pattern_hughes, n_jobs=n_jobs)\n",
    "xfinity_count = count_successful_addresses(pattern_xfinity, n_jobs=n_jobs)\n",
    "viastat_count = count_successful_addresses(pattern_viastat, n_jobs=n_jobs)\n",
    "all_records = hughes_count + xfinity_count + viastat_count\n",
    "\n",
    "print(f\"\"\"Hughes Net: {hughes_count}\n",
    "Xfinity: {xfinity_count}\n",
    "ViaStat: {viastat_count}\n",
    "Total: {all_records}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions we're going to be using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `check_redlining` grades by looking if an addresses' coordinates (converted to a Shapely `Point`) are within the `Polygon`s of redlining maps by Mapping Inequality. This actual check is done by `get_holc_grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_holc_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hughes Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_hughes) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_hughes = []\n",
    "    files = glob.glob(pattern_hughes)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `hughes_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_hughes.extend(record)\n",
    "    hughes = pd.DataFrame(data_hughes)\n",
    "    del data_hughes\n",
    "    \n",
    "    \n",
    "    hughes['block_group'] = hughes['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    hughes = check_redlining(hughes)\n",
    "    # merge census data, and save the file\n",
    "    hughes_acs = hughes.merge(acs[acs_cols], how='left',\n",
    "                        left_on='block_group', right_on='geoid')\n",
    "    hughes_acs = hughes_acs[[c for c in hughes_acs.columns if c != 'geoid']]\n",
    "    hughes_acs.to_csv(fn_hughes, index=False, compression='gzip')\n",
    "else:\n",
    "    hughes_acs = pd.read_csv(fn_hughes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(hughes_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(hughes_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hughes_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(hughes_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hughes_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xfinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_xfinity) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_xfinity = []\n",
    "    files = glob.glob(pattern_xfinity)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `isp_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_xfinity.extend(record)\n",
    "    xfinity = pd.DataFrame(data_xfinity)\n",
    "    del data_xfinity\n",
    "    \n",
    "    \n",
    "    xfinity['block_group'] = xfinity['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    xfinity = check_redlining(xfinity)\n",
    "    # merge census data, and save the file\n",
    "    xfinity_acs = xfinity.merge(acs[acs_cols], how='left',\n",
    "                        left_on='block_group', right_on='geoid')\n",
    "    xfinity_acs = xfinity_acs[[c for c in xfinity_acs.columns if c != 'geoid']]\n",
    "    xfinity_acs.to_csv(fn_xfinity, index=False, compression='gzip')\n",
    "else:\n",
    "    xfinity_acs = pd.read_csv(fn_xfinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(xfinity_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(xfinity_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xfinity_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(xfinity_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfinity_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viastat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_viastat) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_viastat = []\n",
    "    files = glob.glob(pattern_viastat)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `isp_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_viastat.extend(record)\n",
    "    viastat = pd.DataFrame(data_viastat)\n",
    "    del data_viastat\n",
    "    \n",
    "    \n",
    "    viastat['block_group'] = viastat['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    viastat = check_redlining(viastat)\n",
    "    # merge census data, and save the file\n",
    "    viastat_acs = viastat.merge(acs[acs_cols], how='left',\n",
    "                        left_on='block_group', right_on='geoid')\n",
    "    viastat_acs = viastat_acs[[c for c in viastat_acs.columns if c != 'geoid']]\n",
    "    viastat_acs.to_csv(fn_viastat, index=False, compression='gzip')\n",
    "else:\n",
    "    viastat_acs = pd.read_csv(fn_viastat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(viastat_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(viastat_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(viastat_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(viastat_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viastat_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
