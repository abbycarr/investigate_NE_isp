{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Processing Lookup Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import multiprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from parsers import (\n",
    "    hughes_workflow,\n",
    "    get_incorporated_places, \n",
    "    check_redlining, \n",
    "    get_holc_grade, \n",
    "    get_closest_fiber\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "fn_acs = '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'\n",
    "pattern_hughes = '../data/intermediary/isp/hughes/*/*.geojson.gz' # pattern for all data collected from lookup tools\n",
    "pattern_xfinity = '../data/intermediary/isp/xfinity/*/*.geojson.gz'\n",
    "pattern_viastat =  \"../data/intermediary/isp/viastat/*/*.geojson.gz\"\n",
    "\n",
    "# outputs\n",
    "fn_hughes = \"../data/output/speed_price_hughes.csv.gz\"\n",
    "fn_xfinity = '../data/output/speed_price_xfinity.csv.gz'\n",
    "fn_viastat = '../data/output/speed_price_viastat.csv.gz'\n",
    "\n",
    "# params\n",
    "n_jobs = 20\n",
    "recalculate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\J-Dog\\23FALL\\Algorithm Audits\\investigate_NE_isp\\notebooks\\1-process-offers.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# This is from Census data we crunched in the previous notebook.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m acs \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(fn_acs, dtype\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mgeoid\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mblock_group\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mstr\u001b[39;49m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# These are the columns we're going to bring to merge with lookup responses.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m acs_cols \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgeoid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrace_perc_non_white\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mincome_lmi\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mppl_per_sq_mile\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mn_providers\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mincome_dollars_below_median\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39minternet_perc_broadband\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmedian_household_income\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/J-Dog/23FALL/Algorithm%20Audits/investigate_NE_isp/notebooks/1-process-offers.ipynb#Y104sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:755\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39mif\u001b[39;00m compression \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgzip\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    752\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    753\u001b[0m         \u001b[39m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[0;32m    754\u001b[0m         \u001b[39m# \"GzipFile\", variable has type \"Union[str, BaseBuffer]\")\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m         handle \u001b[39m=\u001b[39m gzip\u001b[39m.\u001b[39mGzipFile(  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    756\u001b[0m             filename\u001b[39m=\u001b[39mhandle,\n\u001b[0;32m    757\u001b[0m             mode\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m    758\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompression_args,\n\u001b[0;32m    759\u001b[0m         )\n\u001b[0;32m    760\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    761\u001b[0m         handle \u001b[39m=\u001b[39m gzip\u001b[39m.\u001b[39mGzipFile(\n\u001b[0;32m    762\u001b[0m             \u001b[39m# No overload variant of \"GzipFile\" matches argument types\u001b[39;00m\n\u001b[0;32m    763\u001b[0m             \u001b[39m# \"Union[str, BaseBuffer]\", \"str\", \"Dict[str, Any]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompression_args,\n\u001b[0;32m    767\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\J-Dog\\AppData\\Local\\Programs\\Python\\Python39\\lib\\gzip.py:173\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    171\u001b[0m     mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m fileobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     fileobj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmyfileobj \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, mode \u001b[39mor\u001b[39;49;00m \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    174\u001b[0m \u001b[39mif\u001b[39;00m filename \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fileobj, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/intermediary/census/aggregated_tables_plus_features.csv.gz'"
     ]
    }
   ],
   "source": [
    "# This is from Census data we crunched in the previous notebook.\n",
    "acs = pd.read_csv(fn_acs, dtype={'geoid': str, 'block_group': str})\n",
    "\n",
    "# These are the columns we're going to bring to merge with lookup responses.\n",
    "acs_cols = [\n",
    "    'geoid', 'race_perc_non_white','income_lmi', \n",
    "    'ppl_per_sq_mile', 'n_providers', 'income_dollars_below_median',\n",
    "    'internet_perc_broadband', 'median_household_income'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_addresses(fn):\n",
    "    \"\"\"\n",
    "    How many addresses did we successfully collect in each file?\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    import json\n",
    "    count = 0\n",
    "    with gzip.open(fn, 'rb') as f:\n",
    "        for line in f.readlines():\n",
    "            record = json.loads(line)\n",
    "            count += 1\n",
    "    return count \n",
    "\n",
    "def count_successful_addresses(pattern, n_jobs=20):\n",
    "    \"\"\"\n",
    "    For all files in `pattern`, sees how many addresses were successfully counted.\n",
    "    Uses multiprocessing to speed things up.\n",
    "    \"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    count = 0\n",
    "    with multiprocess.get_context(\"spawn\").Pool(n_jobs) as pool:\n",
    "        for _count in tqdm(pool.imap_unordered(count_addresses, files), \n",
    "                           total=len(files)):\n",
    "            count += _count\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 14.34it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  7.40it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hughes Net: 7\n",
      "Xfinity: 3\n",
      "ViaStat: 0\n",
      "Total: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hughes_count = count_successful_addresses(pattern_hughes, n_jobs=n_jobs)\n",
    "xfinity_count = count_successful_addresses(pattern_xfinity, n_jobs=n_jobs)\n",
    "viastat_count = count_successful_addresses(pattern_viastat, n_jobs=n_jobs)\n",
    "all_records = hughes_count + xfinity_count + viastat_count\n",
    "\n",
    "print(f\"\"\"Hughes Net: {hughes_count}\n",
    "Xfinity: {xfinity_count}\n",
    "ViaStat: {viastat_count}\n",
    "Total: {all_records}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions we're going to be using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We `check_redlining` grades by looking if an addresses' coordinates (converted to a Shapely `Point`) are within the `Polygon`s of redlining maps by Mapping Inequality. This actual check is done by `get_holc_grade`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m \u001b[0mget_holc_grade\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolygons\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSource:\u001b[0m   \n",
      "\u001b[1;32mdef\u001b[0m \u001b[0mget_holc_grade\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m\n",
      "\u001b[0m                   \u001b[0mpolygons\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;34m\"\"\"\n",
      "    Converts any lat and lon in a dictionary into a shapely point,\n",
      "    then iterate through a list of dictionaries containing \n",
      "    shapely polygons shapes for each HOLC-graded area.\n",
      "    \"\"\"\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mfor\u001b[0m \u001b[0mpolygon\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpolygons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
      "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mpolygon\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'grade'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\j-dog\\23fall\\algorithm audits\\investigate_ne_isp\\notebooks\\parsers.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "??get_holc_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hughes Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(fn_hughes) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_hughes = []\n",
    "    files = glob.glob(pattern_hughes)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `hughes_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(hughes_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_hughes.extend(record)\n",
    "    hughes = pd.DataFrame(data_hughes)\n",
    "    del data_hughes\n",
    "    \n",
    "    \n",
    "    hughes['block_group'] = hughes['block_group'].apply(lambda x: f\"{int(x):012d}\")\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    hughes = check_redlining(hughes)\n",
    "    # merge census data, and save the file\n",
    "    hughes_acs = hughes.merge(acs[acs_cols], how='left',\n",
    "                        left_on='block_group', right_on='geoid')\n",
    "    hughes_acs = hughes_acs[[c for c in hughes_acs.columns if c != 'geoid']]\n",
    "    hughes_acs.to_csv(fn_hughes, index=False, compression='gzip')\n",
    "else:\n",
    "    hughes_acs = pd.read_csv(fn_hughes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2023, 12, 4, 14, 2, 7, 492361),\n",
       " datetime.datetime(2023, 12, 4, 14, 4, 8, 780826)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(hughes_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(hughes_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hughes_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.extend(hughes_acs['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redlining_grade\n",
       "C    0.583333\n",
       "D    0.333333\n",
       "B    0.083333\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hughes_acs.redlining_grade.value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
