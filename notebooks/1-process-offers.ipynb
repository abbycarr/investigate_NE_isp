{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing and Processing Lookup Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to analyze the offers scraped for each provider and join each offer scraped with census information about eahc address the offer was scraped for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import multiprocess\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from parsers import (\n",
    "    isp_workflow,\n",
    "    check_redlining\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "fn_acs = '../data/census/aggregated_tables_plus_features.csv.gz'\n",
    "pattern_hughes = '../data/intermediary/offers/hughes/*/*.geojson.gz' # pattern for all data collected from lookup tools\n",
    "pattern_xfinity = '../data/intermediary/offers/xfinity/*/*.geojson.gz'\n",
    "pattern_viasat =  \"../data/intermediary/offers/viasat/*/*.geojson.gz\"\n",
    "\n",
    "# outputs\n",
    "fn_hughes = \"../data/output/speed_price_hughes.csv.gz\"\n",
    "fn_xfinity = '../data/output/speed_price_xfinity.csv.gz'\n",
    "fn_viasat = '../data/output/speed_price_viasat.csv.gz'\n",
    "\n",
    "# params\n",
    "n_jobs = 20\n",
    "recalculate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is from Census data we crunched in the previous notebook.\n",
    "acs = pd.read_csv(fn_acs, dtype={'geoid': str, 'block_group': str})\n",
    "\n",
    "# These are the columns we're going to bring to merge with lookup responses.\n",
    "acs_cols = [\n",
    "    'geoid', 'race_perc_non_white','income_lmi', \n",
    "    'ppl_per_sq_mile', 'n_providers', 'income_dollars_below_median',\n",
    "    'internet_perc_broadband', 'median_household_income'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Number of Offers Scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_addresses(fn):\n",
    "    \"\"\"\n",
    "    How many addresses did we successfully collect in each file?\n",
    "    \"\"\"\n",
    "    import gzip\n",
    "    count = 0\n",
    "    with gzip.open(fn, 'rb') as f:\n",
    "        for _ in f.readlines():\n",
    "            count += 1\n",
    "    return count \n",
    "\n",
    "def count_successful_addresses(pattern, n_jobs=20):\n",
    "    \"\"\"\n",
    "    For all files in `pattern`, sees how many addresses were successfully counted.\n",
    "    Uses multiprocessing to speed things up.\n",
    "    \"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    count = 0\n",
    "    with multiprocess.get_context(\"spawn\").Pool(n_jobs) as pool:\n",
    "        for _count in tqdm(pool.imap_unordered(count_addresses, files), \n",
    "                           total=len(files)):\n",
    "            count += _count\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:01<00:00, 629.28it/s]\n",
      "100%|██████████| 276/276 [00:00<00:00, 403.80it/s]\n",
      "100%|██████████| 95/95 [00:00<00:00, 191.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hughes Net: 700\n",
      "Xfinity: 276\n",
      "ViaSat: 95\n",
      "Total: 1071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print number of offers collected for each ISP\n",
    "hughes_count = count_successful_addresses(pattern_hughes, n_jobs=n_jobs)\n",
    "xfinity_count = count_successful_addresses(pattern_xfinity, n_jobs=n_jobs)\n",
    "viasat_count = count_successful_addresses(pattern_viasat, n_jobs=n_jobs)\n",
    "all_records = hughes_count + xfinity_count + viasat_count\n",
    "\n",
    "print(f\"\"\"Hughes Net: {hughes_count}\n",
    "Xfinity: {xfinity_count}\n",
    "ViaSat: {viasat_count}\n",
    "Total: {all_records}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hughes Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:11<00:00, 62.73it/s] \n",
      "100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_hughes) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_hughes = []\n",
    "    files = glob.glob(pattern_hughes)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `hughes_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_hughes.extend(record)\n",
    "    hughes = pd.DataFrame(data_hughes)\n",
    "    del data_hughes\n",
    "    \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    hughes = check_redlining(hughes)\n",
    "    # merge census data, and save the file\n",
    "    hughes_acs = hughes.merge(acs[acs_cols], how='left',\n",
    "                        left_on='geoid', right_on='geoid')\n",
    "    hughes_acs = hughes_acs[[c for c in hughes_acs.columns if c != 'geoid']]\n",
    "    hughes_acs.to_csv(fn_hughes, index=False, compression='gzip')\n",
    "else:\n",
    "    hughes_acs = pd.read_csv(fn_hughes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2023, 12, 5, 19, 15, 55, 406230),\n",
       " datetime.datetime(2023, 12, 5, 21, 56, 55, 37173)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(hughes_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(hughes_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hughes_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redlining_grade\n",
       "C    0.714719\n",
       "D    0.186646\n",
       "B    0.097117\n",
       "A    0.001517\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hughes_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xfinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:10<00:00, 26.85it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_xfinity) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_xfinity = []\n",
    "    files = glob.glob(pattern_xfinity)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `isp_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_xfinity.extend(record)\n",
    "    xfinity = pd.DataFrame(data_xfinity)\n",
    "    del data_xfinity\n",
    "        \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    xfinity = check_redlining(xfinity)\n",
    "    # merge census data, and save the file\n",
    "    xfinity_acs = xfinity.merge(acs[acs_cols], how='left',\n",
    "                        left_on='geoid', right_on='geoid')\n",
    "    xfinity_acs = xfinity_acs[[c for c in xfinity_acs.columns if c != 'geoid']]\n",
    "    xfinity_acs.to_csv(fn_xfinity, index=False, compression='gzip')\n",
    "else:\n",
    "    xfinity_acs = pd.read_csv(fn_xfinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2023, 12, 5, 19, 16, 14, 337115),\n",
       " datetime.datetime(2023, 12, 5, 21, 56, 50, 758039)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(xfinity_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(xfinity_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xfinity_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redlining_grade\n",
       "C    0.678431\n",
       "D    0.203922\n",
       "B    0.117647\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xfinity_acs.redlining_grade.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viasat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [00:10<00:00,  9.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 13.71it/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(fn_viasat) or recalculate:\n",
    "    # find the data we collected for each block group.\n",
    "    data_viasat = []\n",
    "    files = glob.glob(pattern_viasat)\n",
    "    with multiprocess.Pool(n_jobs) as pool:\n",
    "        # create parallel jobs that parse each block group of data using `isp_workflow`.\n",
    "        for record in tqdm(pool.imap_unordered(isp_workflow, files), \n",
    "                           total=len(files)):\n",
    "            data_viasat.extend(record)\n",
    "    viasat = pd.DataFrame(data_viasat)\n",
    "    del data_viasat\n",
    "        \n",
    "    # check HOLC-grades for each address, and the distance to download speeds at or above 200 Mbps\n",
    "    viasat = check_redlining(viasat)\n",
    "    # merge census data, and save the file\n",
    "    viasat_acs = viasat.merge(acs[acs_cols], how='left',\n",
    "                        left_on='geoid', right_on='geoid')\n",
    "    viasat_acs = viasat_acs[[c for c in viasat_acs.columns if c != 'geoid']]\n",
    "    viasat_acs.to_csv(fn_viasat, index=False, compression='gzip')\n",
    "else:\n",
    "    viasat_acs = pd.read_csv(fn_viasat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2023, 12, 5, 19, 16, 2, 321100),\n",
       " datetime.datetime(2023, 12, 5, 19, 38, 27, 988119)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start and end collection datetime\n",
    "[datetime.fromtimestamp(viasat_acs.collection_datetime.min()), \n",
    " datetime.fromtimestamp(viasat_acs.collection_datetime.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(viasat_acs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "redlining_grade\n",
       "C    0.609756\n",
       "D    0.329268\n",
       "B    0.060976\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viasat_acs.redlining_grade.value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
